{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains the solution implemented for the \"Navigation\" project based on a Unity's ML-Agents environment. This is the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Solution description\n",
    "\n",
    "The project solution is composed of this same jupyter notebook and three Python files:\n",
    "\n",
    "* dqn_agent: Contains the agent that implements the DQN algorithm for learning.\n",
    "* model: Contains the definition of the model layers and how the data forward is processed from the input layer to the output layer that returns the action to be executed. It also includes a couple of functions to load and save already trained models.\n",
    "* utils: Contains utility functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training phase\n",
    "\n",
    "### 1. Importing the required packages\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from dqn_agent import Agent\n",
    "from utils import moving_average, write_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my particular case I have experienced the following error during model training:\n",
    "\n",
    "```console\n",
    "Disposing session as kernel process died ExitCode: 3, Reason: OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\n",
    "```\n",
    "\n",
    "After a quick search on the Internet I found the reason for the problem and the solution at [Stack Overflow](https://stackoverflow.com/questions/53014306/error-15-initializing-libiomp5-dylib-but-found-libiomp5-dylib-already-initial). The next cell contains the proposed solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround to avoid error #15:\n",
    "# Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized OMP\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting the model and training parameters \n",
    "\n",
    "Next, a series of constants are defined that will allow the model to be parameterized centrally throughout the project. This avoids having to go through different files to configure the parameters and avoids errors in the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 37             # Size for the input states\n",
    "ACTION_SIZE = 4             # Number of possible actions\n",
    "HIDDEN_LAYERS = [64, 32]    # Array for the sizes of the hidden layers\n",
    "# HIDDEN_LAYERS = [256, 128, 64, 32]\n",
    "\n",
    "DROP_P = 0.15               # Probability for the dropout layers in the QNetwork\n",
    "\n",
    "BUFFER_SIZE = int(1e5)      # Replay buffer size\n",
    "BATCH_SIZE = 64             # Minibatch size\n",
    "UPDATE_EVERY = 4            # Network update frequency\n",
    "\n",
    "N_EPISODES = 2000           # Number of episodes\n",
    "MAX_T = 1000                # Number of timesteps per episode\n",
    "TRAINING_GOAL = 13.0        # Training goal for the scores mean in an episode\n",
    "\n",
    "EPS_START = 1.0             # Epsilon initial value\n",
    "EPS_END =0.1                # Minimum value for Epsilon\n",
    "EPS_DECAY = 0.995           # Epsilon decay factor\n",
    "GAMMA = 0.99                # Discount factor\n",
    "TAU = 1e-3                  # For soft update of target parameters\n",
    "LR = 5e-4                   # Learning rate \n",
    "\n",
    "SEED = 777                  # Seed for the random generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuring where the model will run\n",
    "GPU processing is configured whenever possible, a message indicates whether the model will be trained/executed on the CPU or on the GPU using CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for the processing.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for the processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Starting up the environment\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Unity environment is instantiated\n",
    "environment = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\Banana.exe\")\n",
    "agent = Agent(state_size=STATE_SIZE, action_size=ACTION_SIZE, hidden_layers=HIDDEN_LAYERS, drop_p=DROP_P, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU, lr=LR, update_every=UPDATE_EVERY, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Examining the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = environment.brain_names[0]\n",
    "brain = environment.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = environment.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Running the episodes\n",
    "\n",
    "The agent executes successive episodes for training until the objective specified in the goal parameter is reached, i.e. an equal or higher value for the average score within an episode is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(env, n_episodes, max_t, eps_start, eps_end, eps_decay, goal):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "        \n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "        \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps).astype(int)     # the agent choose an action (casted to int to avoid runtime error)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done) \n",
    "            state = next_state \n",
    "            score += reward                           # update the scores\n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "            \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        \n",
    "        if write_scores(i_episode=i_episode, scores_window=scores_window, eps=eps, goal=goal):\n",
    "            agent.qnetwork_local.save(f\"Solver_{math.trunc(datetime.timestamp(datetime.now()))}.pth\")\n",
    "            break\n",
    "           \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training the model\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!\n",
    "\n",
    "When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.89\tEpsilon: 0.6057704364907278\n",
      "Episode 200\tAverage Score: 5.01\tEpsilon: 0.36695782172616715\n",
      "Episode 300\tAverage Score: 8.19\tEpsilon: 0.22229219984074702\n",
      "Episode 400\tAverage Score: 10.66\tEpsilon: 0.13465804292601349\n",
      "Episode 453\tAverage Score: 13.00\tEpsilon: 0.10324180238648367\n",
      "Environment solved in 453 episodes!\tAverage Score: 13.00\n",
      "Model saved to model_1651259749.pth.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmIUlEQVR4nO3deXhU5f3+8fcnKwECJBAWgRA2AUXWgCKu0LrXrVah7ktxK9LWtm7tT1trv11t1WoVFVS0gLvWrYobKrJFdtkhZAECgSSEJGR9fn/MgWJKIEJmzkzmfl0XV2bOTHJuj+HmzHPOeY455xARkegR43cAEREJLRW/iEiUUfGLiEQZFb+ISJRR8YuIRJk4vwM0RocOHVxGRobfMUREIkpWVlahcy6t/vKIKP6MjAwWLlzodwwRkYhiZpsOtFxDPSIiUUbFLyISZVT8IiJRRsUvIhJlVPwiIlFGxS8iEmVU/CIiUUbFLyIShsoqa7jvzRWUVFQ3+c9W8YuIhJk56woZ/+Rcnvsym6xNO5v850fElbsiItHi9UX5/PTFxbRvlcBjlw9nTP9OTb4OFb+ISBj4ePU2ZszPYdbKbZzQsz1Trx1Bi/jYoKxLxS8i4rNleSXcOC2LpPhYTu/XkYfGDQla6YOKX0TEd39+fzXJiXF8ePuptGuZEPT16eCuiIiP3l22hdlrtnP9yT1DUvqg4hcR8c3GwjImTl/E0PR2XD0qI2Tr1VCPiEiI5ewoZ3FeMU9/vpEYMyZfmUmrxNDVsYpfRCREyqtqeHL2Rh7/dD0V1bUA/GB4N9KSE0OaQ8UvIhIiD7y9khfm5TC2f0d++t2j2VVRzXHd2oY8h4pfRCQEcnaUM3NBLlee0IP7LxzoaxYd3BURCYEX5m/CAbee3sfvKCp+EZFgq6qp45WsPMb270jnti38jhO84jezKWa2zcyW77fsz2a2ysyWmtlrZtYuWOsXEQkXH60qoHB3FeNGdvc7ChDcPf5ngLPqLfsAGOicGwSsAe4K4vpFRMLCjAW5dG7TglP6pvkdBQjiwV3n3Gwzy6i37P39ns4FLgnW+kVE/OSc4+bnv2J1QSkbC8uYNLYvcbHhMbru51k91wEzfVy/iEjQPPflJt5bsRWAC4YcFRYHdffypfjN7B6gBnjhIO+ZAEwASE9PD1EyEZEj9/bSLdz75gpO65fGlKtHEBNjfkf6hpB/7jCza4DzgMudc66h9znnJjvnMp1zmWlp4TEuJiJyKCUV1dz37xUM7taWx68YHnalDyHe4zezs4BfAqc658pDuW4RkWDbXFzBFU/NY2dZFU9dlRnUOfWPRDBP55wOfAn0M7M8M7se+AeQDHxgZovN7PFgrV9EJJRyd5Zz9ZT5bC+t5IUbjmdw93Z+R2pQMM/qGX+AxU8Ha30iIqHgnOPNJZvpndaaypo6hqW3Y3PJHi567Asqa+p44qrhnNCrvd8xD0pz9YiINNKcdYVM+WIjs1Zu27fs2tEZLMjeSWV1Ha/eciJ9OyX7mLBxVPwiIo1QUl7N9c8upKK6lqHp7Thv0FGsLShl6hfZmMHTV2dGROmDil9EpFEe+3QdFdW1TLkmk5P7phHvXYx1/pCjqKiqZUz/Tj4nbDwVv4jIATjnWLW1lBWbdzFzQQ4Lsou4ZHi3/yn4E3t38Cnh4VPxi4jUs3prKZNmLGLV1lIA+nZszaSxfZk4Jnyuvj0SKn4Rkf2UV9Vw5dPzcMAfLj6OXmmtGd4jhdgwvBDrcKn4RUT2M/WLbLaVVvLyTaPIzEj1O05QhMdUcSIiYaCkvJonPl3P2P4dm23pg4pfRGSfRz5aS2llDT8/s5/fUYJKQz0iEvUW5RRRVlnLlC82Mm5EOgO6tPE7UlCp+EUkqi3KKeKix+YA0KN9S+4+p7/PiYJPQz0iEtX23ixlREYKz1w7kuQW8T4nCj7t8YtIVFqeX8KinCLeWrKFk/t2YNr1x/sdKWRU/CISdZ6dk81v3/qa2jpHYlwMd58zwO9IIaXiF5FmpayyhpYJsZgFLrjaU12LGVTV1HHvGyv4aPU2isur+e4xnbjv/GPplJwYNjdBDxUVv4g0G4tyihg3eS4Du7YlrXUiCzcVUVZZQ7eUJLqntuTTNdu5aGhX+ndO5poTM6Ku8PdS8YtIxHPO8du3vmbG/Fwqa+rIKypn1ZZdjBnQiaT4GN5euoW123bzq3MHcMPJvfyO6zsVv4hEtKc+28Dv3l4JwNkDO3P7Gf3o07E1zrl9wz2/Ou8Yamodqa0S/IwaNlT8IhKxSvdU8/dZawE4uW8H/vHDYfsmU9tb+gBtouAUzW9DxS8iEWvmglx2V9bw5o9HM6hbO7/jRIzoPLIhIhHPOccL83IY3iNFpf8taY9fRCJKTW0dry/ezL/mbWJjYVmzuTlKKKn4RSQi7Cyr4pcvL+HzdYXsqa6jd1orfnXuAC4c0tXvaBFHxS8iYW1zcQXPz93E/I07WZpfwqWZ3TipTxpnHtvpGwdwpfGCVvxmNgU4D9jmnBvoLUsFZgIZQDZwqXOuKFgZRCRyOeeYsSCXJ2dvYENhGQC/OLMft56uoZ0jFcyDu88AZ9VbdifwoXOuL/Ch91xEZB/nHDc/n8XoP3zEXa8uo6bOcdvYvpxxTCeuHZ3hd7xmIWh7/M652WaWUW/xBcBp3uNngU+AO4KVQUQii3OOl7LyeHd5YKrkHx6fzgMXDtSQThML9Rh/J+fcFu/xVqBTQ280swnABID09PQQRBMRP2RtKqJbShIG/Pzlpcxes53B3dvxyk2jonYunWDz7eCuc86ZmTvI65OByQCZmZkNvk9EIs9bSzfz4sI8/t95Axg3+Ut6p7UmxozsHWX8+rxjuPKEHir9IAp18ReYWRfn3BYz6wJsC/H6RcRHtXWOf83P4devLwfgOw9uB2DV1lIAnrl2BKf16+hbvmgR6uJ/E7ga+IP39Y0Qr19EfPTvJZv3lX5mjxTWFJTys+8ezfrtZZx5bGdO6tvB54TRIZinc04ncCC3g5nlAfcSKPwXzex6YBNwabDWLyLh56WsXNq0iOP1W0fTs0MrnIOYGB24DbVgntUzvoGXxgZrnSISvlZt3cWc9TuYNLYvvdJaA6CTdfyhK3dFJKh27anm30s2M+3LTbRNiufqURl+R4p6Kn4RCZrXF+Xzm3+voKi8ms5tWvD7i44jRTdD8Z2KX0SAwBk3X+UUUVxezZj+HYmNMYrKqnjow7W8nJXH2QM7c/+FA2kRH3vQn1O6p5ppczexemspbyzezPAeKfz6vGMY3K2tLsQKEyp+EWFzcQXjJs8lZ2c5AKN6tadFfAyfrgmcbnnK0Wm8lJUHwO1n9CO3qJwX5m7ishHpjOrdHoCVW3axPL+Exz5Zz8bCMszgltN687PvHq1z8sOMil8kSuyprqWovIpYM9KSE9lZVsWuPTV0S0li4vRF7NhdySPjh1JSUc0f311FdV0dN57amwuHdKVf52T++N4q/vnJ+n3/AAB8smY7z147ktUFpdztzavTqU0iMyecwKBu7UhKOPinA/GHORf+F8VmZma6hQsX+h1DJGLV1TnO/Pts1m7bDUDnNi3YVrqHOgeDurVlaV4JD48fyvmDjwKguLyKiupaurRN2vcznHMsyi1mxeZdpLZMoGtKEj96biHbSysxg5P6dOCXZ/Yno0NLknWP27BgZlnOucz6y7XHLxIFvlhfyNptu7ludE+OateCr3KKSGmZwMtZeSzNK2HCKb32lT5Au5YJtKv3M8yMYekpDEtP2bfso9tP5YV5OWQXBqZaaJWoSokE+r8k0szV1Tn++cl6UlslcMfZ/UiM++/wy02n9qa4vJrjurU9rJ+d3CKem07t3VRRJURU/CLNmHOOv7y/mjnrd/C7Cwd+o/QBuqe2pHuqT+HENzrULtKMPf7pBh77ZD3jRnTn8uM1vbkEaI9fpBlxzvHqV/kUlO4hd2cFMxfkcO6gLvz+ouN0Dr3so+IXaQacc7yzbCszF+Yy2zv3vmVCLBcO7cpvLxioidDkG1T8Is3AY5+s58//Wc1RbVswaWxfrjupJ60T44hV4csBqPhFItyO3ZU89vE6vjOgE09cOVxlL4ekg7siEWZb6R7eW76VddsCd6164O2VVNbUcefZ/VT60ija4xeJMD+ZsZg563cAkNIynqLyam4b04c+HZN9TiaRQsUvEkHWbdvNnPU7uGR4NwYe1YZl+bsYkt6O8SO6+x1NIoiKXySCTJ+fQ1yMccdZ/UlLTvQ7jkQoFb9ImJv2ZTazVm4jr6ic9dvLOHdQF5W+HBEVv0iYWltQypOfbeDFhXn06diagl2VALoCV46Yil8kDFVU1XLN1AXkF1cwsGsbXrtlNOVVtSzKKWJUr/Z+x5MIp+IXCTPbSyu545Wl5BdXcP8Fx3L+kK7Ex8bQNimG0/p19DueNAMqfpEwsq10D9dOXcC6bbv51bkDuHJUht+RpBlS8YuEgdo6x2//vYIXF+ZRW+d44qrhnK69ewkSX4rfzH4K3AA4YBlwrXNujx9ZREJhT3UtCbEx/zNZ2pLcYpZvLmHVllKmzd3E94d1Y+KYPmR0aOVTUokGIS9+M+sK3AYc45yrMLMXgXHAM6HOIhJs20sr+cXLS/hk9XZ6pbXiH+OHkdwijneXbyGlZQJ3vrqM2rrAfa8vPz6dBy46zufEEg38GuqJA5LMrBpoCWz2KYdI0FTV1PHjf33FkrxiJpzSi5kLcjnn4c++8Z5+nZL566WDaREfoykXJGRCXvzOuXwz+wuQA1QA7zvn3q//PjObAEwASE/XecsSGeZv3MlXOUV8Z0AnfvHyEhblFPPgpYO5eFg3rjkxg9lrtlNWVcvaglJmLMjlzrP7M7Dr4d3vVuRwmXMutCs0SwFeAS4DioGXgJedc8839D2ZmZlu4cKFoQkocpicc4z566dsLCwDIC7GeGjcUM4d1OWA780vrqBbSstQx5QoYmZZzrnM+sv9GOr5DrDRObcdwMxeBU4EGix+kUjw0aptbCws46ZTe1NcXsVp/dI4a+D/lj6Aman0xTd+FH8OcIKZtSQw1DMW0O68RLQ/vreKf36ynrZJ8dx6em+SW8T7HUmkQX6M8c8zs5eBr4AaYBEwOdQ5RJpCWWUNk2YsZtbKAn4wvBu3n9FPpS9hz5ezepxz9wL3+rFukabywdcF/OPjdSzPL+GOs/rzo5N7Eherm9pJ+Gt08ZtZEpDunFsdxDwiEWFRThE3PZ9F+1YJ/PmSQVw8rJvfkUQarVG7J2b2PWAx8J73fIiZvRnEXCJhq6yyhp/OXEznNi344GenqvQl4jT2c+l9wEgCp1/inFsM9AxKIpEwtr20khueXcimneX89dLBtE3SeL5EnsYO9VQ750rMvjHPSGgvABDxgXOOiupaWibEMXfDDiZOX8Suimr+fMlgTtC8+BKhGlv8K8zsh0CsmfUlMNfOnODFEgmNssoaluaVcEyXNrRtGU9lTS2rt5YyoEsb9lTXcvlT81iaV0L/zsmsLiilZ/tWPHfdSAZ0aeN3dJHD1tjinwjcA1QC/wL+A/wuWKFEgq2iqpaXsnJ55otsNhSWER9rtE6Mo6SimjoHw3uk0L5VAis27+K60T1ZklfMdaN78tPvHk3rRM1mLpHtkL/BZhYLvO2cO51A+YtEtOraOq6ZOp95G3cCMH5kOq0TYyndU0P71gm0TYrnL/9ZQ1VtHRPH9OH2M/r5nFikaR2y+J1ztWZWZ2ZtnXMloQglEkzPzslm3sad3H/BsfTpmMwJvVKpd/yK9NRWvL4on5tP6+1TSpHgaexn1t3AMjP7ACjbu9A5d1tQUokEyfbSSh6atZbT+6Ud9LaGZw3szFkDO4cumEgINbb4X/X+iESsHbsDNzHfU1PLr887xu84Ir5pVPE75541swTgaG/RaudcdfBiiTStxbnF/Oi5hewsq+Kus/vTK62135FEfNOo4jez04BngWzAgO5mdrVzbnbQkok0kZwd5VwzdT5tWsTz1sSTdCqmRL3GDvX8FThj7zw9ZnY0MB0YHqxgIk1l6pyNlFfW8sato+nRXjcxF2nslA3x+0/O5pxbA+hadQlrVTV1/N87K5n6RTan909T6Yt4GrvHv9DMnuK/d8m6HN08RcLc9Pk5PDF7AwAXDe3qcxqR8NHY4r8ZuJXAVA0AnwGPBSWRSBPYXlrJwx+uZVh6O+48ewAjMlL8jiQSNhpb/HHAQ865B2Hf1byJQUslcgTWFpTysxeXUFZVw/9dPIh+nZP9jiQSVho7xv8hkLTf8yRgVtPHETky1bV1TJiWRV5ROQ+NG6rSFzmAxu7xt3DO7d77xDm327tZukjY+Gztdu55bTk5O8t56qpMvnNMJ78jiYSlxu7xl5nZsL1PzCwTqAhOJJFv78v1O7jumQXUOcdVo3owdkBHvyOJhK3G7vH/BHjJzDZ7z7sAlwUlkci3sLuyhtyd5UycvojuqS157ZbRuiuWyCEctPjNbASQ65xbYGb9gRuBiwnce3djCPKJ7FNSXk2bpLh9M2kW7q7kvIc/Z+uuPaS0jOfxK4ar9EUa4VBDPU8AVd7jUcDdwKNAETA5iLlEvuG95VsY8cAsbno+i/ziCj5aVcBN07LYvruS60b35NVbRnN0Jx3IFWmMQw31xDrndnqPLwMmO+deAV4xs8WHu1Izawc8BQwkcO/e65xzXx7uz5Pm7cUFudz56lLSU1vynxUF/GdFAQBxMcYj44dyznFdfE4oElkOWfxmFuecqwHGAhO+xfcezEPAe865S7xZP3WGkBzQH95dxeOfrufkvh14/IrhrNyyiw9WFjC6dwe6piTRW7Nsinxrhyrv6cCnZlZI4CyezwDMrA9wWHfjMrO2wCnANQDOuSr+O5wkss+jH6/j8U/X88Pj0/nt+ccSFxtDZkYqmRmpfkcTiWgHLX7n3ANm9iGBs3jed84576UYAjdgPxw9ge3AVDMbDGQBk5xzZQf/NokG20r38PzcHDYWlvHvJZu5YMhR3H/BQGJj7NDfLCKN0ph77s49wLI1R7jOYcBE59w8M3sIuBP49f5vMrMJeENL6enpR7A6iRS5O8u58NEv2FleRYwZN5/Wm5+f0U+lL9LEjmSc/nDlAXnOuXne85cJFP83OOcm4505lJmZ6eq/Ls2Dc468osC1gBOmZVFVW8d7k06hV1or4mMbe32hiHwbIS9+59xWM8s1s37eHP9jga9DnUPCw/T5udz92jIAYgymXjtS8+uIBJkfe/wQOD7wgndGzwbgWp9yiE+cczgHT362gfatEpg4pg99OiZzUt8OfkcTafZ8KX7n3GIg0491i/9yd5Zz9ZT5FOzaQ1lVLY+MH8r3Bh/ldyyRqOHXHr9EqcqaWn703EIKd1dy/pCunNArlfMG6QIskVBS8UtI5O4sZ/r8HDZsL2PV1lKmXjOC0/trBk0RP6j4Jehq6xwTpy9icW4xiXEx3Damj0pfxEcqfgm6Z+Zkszi3mAcvHcw5x3WhRXys35FEopqKX4JqS0kFD76/mtP6pXHR0K77plQWEf+o+KXJOOeYt3Eny/NLGNStHZk9Urj3jRXUOsf9FwxU6YuECRW/NInC3ZXc++YK3l66Zd+ylJbxFJVXc/c5/emeqglYRcKFil+O2I7dlZz5t9kUV1TzizP78YPh3ZizfgevLcons0cKPzq5l98RRWQ/Kn45LNW1dWwt2UNifAx/eHcVReVVvHHrSRzXrS0AFw7tyoVDu/qcUkQORMUv35pzjkkzFvHOsq0kxMVQVVPHlSf02Ff6IhLeVPzyrb2xeDPvLNtKh9aJnHlsJ8aNSOfYo9r4HUtEGknFL99KZU0tj3y0lv6dk3nntpOJ0Vz5IhFHE55LoxXuruS0P3/C+u1l3Da2r0pfJEJpj18a7dGP17GttJIp12Qypn8nv+OIyGHSHr80ypLcYqZ9uYkfDO+m0heJcCp+OaTdlTVMmrGIjsmJ3HX2AL/jiMgR0lCPHNJ9b64gZ2c5MyaMom3LeL/jiMgR0h6/HNQ7y7bwclYePz69DyN7pvodR0SagIpfGlRb5/jLf1bTv3Myt43t63ccEWkiKn5p0L/m57ChsIyJY/oSF6tfFZHmQmP88j9q6xwPzVrDo5+s5+S+HThrYGe/I4lIE1Lxy/+4/62veWZONhcMOYrfXTiQWF2oJdKsqPjlGxblFPHMnGyuOTGD+84/1u84IhIEGriVfVZt3cVdry6jfasEfn5mP7/jiEiQ+LbHb2axwEIg3zl3nl85oplzjqc/38jzczdhZuQXV9A6MY4/fX8QrRP1YVCkufLzb/ckYCWg+Xx9MnttIb97eyXH90wltVUCw9JTuPuc/rRvneh3NBEJIl+K38y6AecCDwA/8yNDtCurrOGZLzbSvlUCz10/ksS4WL8jiUiI+LXH/3fgl0ByQ28wswnABID09PTQpIoCdXWOP763imfmZFNZU8fEMX1U+iJRJuTFb2bnAducc1lmdlpD73POTQYmA2RmZrrQpGvevsop4jdvrmBJXgkXD+3K+UOO4pS+aX7HEpEQ82OPfzRwvpmdA7QA2pjZ8865K3zIEhXKKmt47JN1TPk8m9RWCfzh4uO4bER3zHR+vkg0CnnxO+fuAu4C8Pb4f67SD57VW0u5+YUssgvLGDugE7+/6DjSknXwViSa6Zy9Zix3Zzk/eHwOifGxPH/D8ZzYu4PfkUQkDPha/M65T4BP/MzQXL24MJc/vrsKB7xy04mkt2/pdyQRCRO6crcZem/5Fu54ZSm90lox7frjVfoi8g0a6mlmFuUUMWnGYoZ0b8dz1x1PUoJO1RSRb9IefzNSsGsPN07LolObFjx1VaZKX0QOSMXfTNTU1jFhWhZllTU8eVWmpl0QkQZpqKeZeCkrjyW5xTwyfij9Ojd4QbSIiIo/0pXuqebVr/J58IM1DEtvx3mDuvgdSUTCnIo/gi3PL2HCcwvZXLKH4T1S+PMlg3Q1rogckoo/Qq0tKOXqKfNJjIvhpZtGkdkjRaUvIo2i4o9Aq7eWctFjX9AyIZZpNxxP77TWfkcSkQii4o9AD3+4llgz/j3xJLq0TfI7johEGJ3OGWGW5hXzzvItXDmqh0pfRA6Lij+C5BdX8NOZi+mYnMiNp/b2O46IRCgN9USI0j3VXPr4l5RUVDP5quG0TYr3O5KIRCgVfwR4fVE+f3h3FdtK9/DSTScyvEeK35FEJIJpqCeMOeeYt2EH97y2jJo6x19+MFilLyJHTHv8Yai2zjFrZQHvLd/Ka4vyaZ0Yxys3j6JH+1Z+RxORZkDFH2aKyqq4/tkFfJVTDMCNp/bixlN6k9oqwd9gItJsqPjDxLbSPfxrXg4zF+Syo6yKv/5gMGP6dyRFhS8iTUzF77Pl+SXk7iznwQ/WsG77boZ2b8djlw9jaLrG8kUkOFT8PnolK4/bX1oCQFJ8LC9cfzwn9tEN0UUkuFT8PqiqqeOuV5fxyld5nNArlV+deww9O7SiVaL+d4hI8KlpQmjvOP7Hq7ezJLeYH5/ehx+P6UOLeN0iUURCR8UfIoW7Kxk/eS7rt5fRtV0SD48fyvmDj/I7lohEIRV/EDnn+Nustby9dDPF5dWUVdXw4o2jGNkz1e9oIhLFQn7lrpl1N7OPzexrM1thZpNCnSEUauscv39nJQ9/uJbObVswpHs7plw9QqUvIr7zY4+/BrjdOfeVmSUDWWb2gXPuax+yBMXy/BLufHUpy/N3ccUJ6dx/wUDdHUtEwkbIi985twXY4j0uNbOVQFegWRT/7DXbueG5haS0jOeR8UP5nsbxRSTM+DrGb2YZwFBg3gFemwBMAEhPTw9tsG+pqqaO7B1lzFpZwD8+WkfvtNY8f/1I2rdO9DuaiMj/8K34zaw18ArwE+fcrvqvO+cmA5MBMjMzXYjjNVpJRTWXPfElq7aWAnBavzT+9P1BKn0RCVu+FL+ZxRMo/Recc6/6kaEpZBeWceO0LDYU7uY35x/Lib3b07dTst+xREQOKuTFb4GjnE8DK51zD4Z6/U2hpKKat5Zu5u+z1lJTW8eUa0Zwct80v2OJiDSKH3v8o4ErgWVmtthbdrdz7h0fsnxr7y3fyi9eWkJpZQ39OyfzyPih2ssXkYjix1k9nwMReW7jiwtzueOVpQzq1o7fnn8sg7q11WmaIhJxdOVuI1TV1PHRqgJ+9fpyTurTgSevytT8OiISsVT8h+Cc49Z/fcUHXxeQ2iqBBy8dotIXkYim4j+EmQty+eDrAn7ynb5cNSpDt0AUkYin4j+InB3l/Ok/qxmZkcqksX01ni8izYKK/wDeWrqZqV9kszSvmKT4WH574bEqfRFpNlT8wKqtu3j4w7WUV9VSVFbFkrwS+nZszfUn9eLy49PpntrS74giIk0mqos/v7iC5+ZkM3VONq0SYklPbUlSQix3nt2fG07qSVxsyGetFhEJuqgt/pVbdjFu8lxK91Rz/uCjuOfcY0hL1vw6ItL8RWXxryko5Yqn5pEUH8trt5xIr7TWfkcSEQmZqCn+ssoaZi7IZXFuMbNWFtAyIY4XfnS8Sl9Eok6zL/66OsejH69j6pxsdpZV0bVdEmMHdOKecwbQuW0Lv+OJiIRcsy7+Rz5cy0tZeeTsLGds/47cOqYPw9JT/I4lIuKrZl38acmJDOzahutGZ3D1iRk6F19EhGZe/ONGpjNuZHjftlFEJNR0orqISJRR8YuIRBkVv4hIlFHxi4hEGRW/iEiUUfGLiEQZFb+ISJRR8YuIRBlzzvmd4ZDMbDuw6TC/vQNQ2IRxgklZgyeS8iprcERj1h7OubT6CyOi+I+EmS10zmX6naMxlDV4IimvsgaHsv6XhnpERKKMil9EJMpEQ/FP9jvAt6CswRNJeZU1OJTV0+zH+EVE5JuiYY9fRET2o+IXEYkyzbr4zewsM1ttZuvM7E6/89RnZtlmtszMFpvZQm9Zqpl9YGZrva++3CvSzKaY2TYzW77fsgNms4CHve281MyGhUHW+8ws39u2i83snP1eu8vLutrMzgxx1u5m9rGZfW1mK8xskrc87LbtQbKG3bY1sxZmNt/MlnhZf+Mt72lm87xMM80swVue6D1f572eEQZZnzGzjftt1yHe8qb/HXDONcs/QCywHugFJABLgGP8zlUvYzbQod6yPwF3eo/vBP7oU7ZTgGHA8kNlA84B3gUMOAGYFwZZ7wN+foD3HuP9LiQCPb3fkdgQZu0CDPMeJwNrvExht20PkjXstq23fVp7j+OBed72ehEY5y1/HLjZe3wL8Lj3eBwwM4TbtaGszwCXHOD9Tf470Jz3+EcC65xzG5xzVcAM4AKfMzXGBcCz3uNngQv9COGcmw3srLe4oWwXAM+5gLlAOzPrEpKgNJi1IRcAM5xzlc65jcA6Ar8rIeGc2+Kc+8p7XAqsBLoShtv2IFkb4tu29bbPbu9pvPfHAWOAl73l9bfr3u39MjDWQnRT7oNkbUiT/w405+LvCuTu9zyPg//S+sEB75tZlplN8JZ1cs5t8R5vBTr5E+2AGsoWrtv6x95H4yn7DZmFTVZveGEogT2+sN629bJCGG5bM4s1s8XANuADAp84ip1zNQfIsy+r93oJ0N6vrM65vdv1AW+7/s3MEutn9Rzxdm3OxR8JTnLODQPOBm41s1P2f9EFPueF5fm24ZzN80+gNzAE2AL81dc09ZhZa+AV4CfOuV37vxZu2/YAWcNy2zrnap1zQ4BuBD5p9Pc3UcPqZzWzgcBdBDKPAFKBO4K1/uZc/PlA9/2ed/OWhQ3nXL73dRvwGoFf1oK9H+O8r9v8S/g/GsoWdtvaOVfg/eWqA57kv0MOvmc1s3gCRfqCc+5Vb3FYbtsDZQ3nbevlKwY+BkYRGBaJO0CefVm919sCO0Kb9BtZz/KG1pxzrhKYShC3a3Mu/gVAX++ofgKBAzhv+pxpHzNrZWbJex8DZwDLCWS82nvb1cAb/iQ8oIayvQlc5Z19cAJQst+whS/qjYFeRGDbQiDrOO+sjp5AX2B+CHMZ8DSw0jn34H4vhd22bShrOG5bM0szs3be4yTguwSOSXwMXOK9rf523bu9LwE+8j5p+ZV11X7/8BuBYxH7b9em/R0I9hFsP/8QOBq+hsBY3z1+56mXrReBMyCWACv25iMwzvghsBaYBaT6lG86gY/x1QTGFK9vKBuBsw0e9bbzMiAzDLJO87Is9f7idNnv/fd4WVcDZ4c460kEhnGWAou9P+eE47Y9SNaw27bAIGCRl2k58P+85b0I/OOzDngJSPSWt/Cer/Ne7xUGWT/ytuty4Hn+e+ZPk/8OaMoGEZEo05yHekRE5ABU/CIiUUbFLyISZVT8IiJRRsUvIhJlVPzSrJlZ7X6zHS62Q8zSamY3mdlVTbDebDPrcBjfd6aZ/cYCs3W+e6Q5RA4k7tBvEYloFS5waXyjOOceD2KWxjiZwEVHJwOf+5xFmint8UtU8vbI/2SB+yHMN7M+3vL7zOzn3uPbLDAX/VIzm+EtSzWz171lc81skLe8vZm9782v/hSBi272rusKbx2LzewJM4s9QJ7LvEm7bgP+TmAqhGvNLGyuNpfmQ8UvzV1SvaGey/Z7rcQ5dxzwDwJlW9+dwFDn3CDgJm/Zb4BF3rK7gee85fcCnzvnjiUw71I6gJkNAC4DRnufPGqBy+uvyDk3k8Dsl8u9TMu8dZ9/+P/pIgemoR5p7g421DN9v69/O8DrS4EXzOx14HVv2UnA9wGccx95e/ptCNwM5mJv+dtmVuS9fywwHFjgTfeeRMMT7x0NbPAet3KBOfBFmpyKX6KZa+DxXucSKPTvAfeY2XGHsQ4DnnXO3XXQNwVuvdkBiDOzr4Eu3tDPROfcZ4exXpEGaahHotll+339cv8XzCwG6O6c+5jAvOhtgdbAZ3hDNWZ2GlDoAnPUzwZ+6C0/G9h7c5IPgUvMrKP3WqqZ9agfxDmXCbxN4G5LfyIwad8Qlb4Eg/b4pblL8vac93rPObf3lM4UM1sKVALj631fLPC8mbUlsNf+sHOu2MzuA6Z431fOf6f2/Q0w3cxWAHOAHADn3Ndm9isCd1qLITCD6K3ApgNkHUbg4O4twIMHeF2kSWh2TolKZpZNYHrbQr+ziISahnpERKKM9vhFRKKM9vhFRKKMil9EJMqo+EVEooyKX0Qkyqj4RUSizP8HyPWNe0qk7TIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the algorithm execution\n",
    "scores = run_episodes(environment, N_EPISODES, MAX_T, EPS_START, EPS_END, EPS_DECAY, TRAINING_GOAL)\n",
    "\n",
    "# Save the trained model\n",
    "file_path = f\"model_{math.trunc(datetime.timestamp(datetime.now()))}.pth\"\n",
    "agent.qnetwork_local.save(file_path)\n",
    "print(f\"Model saved to {file_path}.\")\n",
    "\n",
    "# Unity environment is closed\n",
    "environment.close()\n",
    "\n",
    "# Moving averages are used on the plot to reduce noise\n",
    "medias = moving_average(scores, 10)\n",
    "\n",
    "# Plot the moving_averages\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "medias = moving_average(scores, 100)\n",
    "plt.plot(np.arange(len(medias)), medias)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Watching the trained model in action\n",
    "\n",
    "The trained model will then be loaded and its performance in the Unity environment will be shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for the processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=37, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=32, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for the processing.\")\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64\\Banana.exe\")\n",
    "\n",
    "file_path = 'Solver_1651259749.pth'\n",
    "agent = Agent()\n",
    "agent.qnetwork_local.load(file_path)\n",
    "agent.qnetwork_local.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 17.0\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = agent.act(state).astype(int)          # Take an action, casted to int to avoid runtime error<<<<<\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions\n",
    "\n",
    "The agent has achieved the set goal with a simple DQN implementation and in a reasonable number of episodes.\n",
    "\n",
    "Durante el proceso de entrenamiento he probado distintas configuraciones para la red neuronal variando tanto el tamaño como el número de las capas ocultas.\n",
    "\n",
    "He comprobado que las configuraciones más sencillas permitían ir subiendo las puntuaciones más rápidamente.\n",
    "\n",
    "Así mismo, he probado con distintos valores para el decremento de epsilon y para el learning rate hasta que he dado con los que me permitían un aprendizaje más lineal y satisfactorio.\n",
    "\n",
    "He incluido dropout layers en el modelo con el fin de evitar overfitting. Tras probar distintos valores para el parámetro drop_p, he llegado a un valor que no limitaba la velocidad de aprendizaje.\n",
    "\n",
    "## 5. Next steps\n",
    "\n",
    "As an exercise on the sidelines of the project, I will implement other versions of the agent that make use of the improvements seen in the nanodegree, such as:\n",
    "\n",
    "* Double DQN (DDQN)\n",
    "* Prioritized experience replay\n",
    "* Dueling DQN\n",
    "\n",
    "I will also approach the exercise from the point of view of the analysis of the frames generated by Unity using torchvision, in order to practice with convolutional neural networks and the application of transformations on the images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
